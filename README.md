# Chapter 7: AI Integration - Giving Your System a Brain

*"The question of whether a machine can think is no more interesting than the question of whether a submarine can swim." - Edsger W. Dijkstra*

---

This is the moment we've been building toward! We've collected tweets, scraped Telegram channels, and parsed RSS feeds. Now it's time to transform that raw data soup into crystal-clear insights using the most powerful AI models available.

In this chapter, we'll integrate **4 different AI providers** - OpenAI, Anthropic, Google, and Ollama - giving you complete flexibility to choose based on your budget, privacy needs, and quality requirements. You'll learn advanced prompt engineering, intelligent content filtering, and how to build AI systems that scale.

## üß† Why 4 Different AI Providers?

**Maximum flexibility for every use case:**

**OpenAI (GPT-4/o1):**
- Excellent reasoning and creative tasks
- Great structured output generation
- Reliable and well-documented
- Premium pricing for premium quality

**Anthropic (Claude):**
- Superior long-form analysis
- Excellent instruction following
- Advanced reasoning capabilities
- Great for complex content analysis

**Google (Gemini):**
- Cost-effective cloud option
- Good performance at lower cost
- Integrated with Google ecosystem
- Great balance of price/performance

**Ollama (Local):**
- Completely free after setup
- Full privacy control (runs locally)
- No API limits or costs
- Perfect for development and testing

**Our Strategy:** Start free with Ollama, scale cost-effectively with Gemini, upgrade to OpenAI/Claude for premium quality when needed.

## üí∞ AI Model Costs & Provider Comparison

We'll support **4 AI providers** to give you flexibility based on your budget and performance needs:

### Cloud Providers (API-based, pay-per-use)

**OpenAI (GPT-4/o1):**
- Input: $2.50 per 1M tokens | Output: $10.00 per 1M tokens
- **Estimate**: ~$0.10-0.50 per digest
- **Best for**: Highest quality reasoning, complex analysis tasks

**Anthropic (Claude):**
- Input: $3.00 per 1M tokens | Output: $15.00 per 1M tokens  
- **Estimate**: ~$0.15-0.75 per digest
- **Best for**: Long-form content analysis, excellent instruction following

**Google (Gemini Pro):**
- Input: $1.25 per 1M tokens | Output: $5.00 per 1M tokens
- **Estimate**: ~$0.05-0.25 per digest
- **Best for**: Cost-effective alternative with good performance

### Local Provider (Self-hosted, free after setup)

**Ollama (Llama 3.1, Qwen, etc.):**
- **Cost**: Free (after initial setup)
- **Hardware**: Requires 8GB+ RAM for good performance
- **Best for**: Development, testing, privacy-sensitive use cases

### Choosing Your Provider

| Provider | Cost | Quality | Speed | Privacy | Best Use Case |
|----------|------|---------|-------|---------|---------------|
| **OpenAI** | $$$$$ | Excellent | Fast | Cloud | Production, highest quality |
| **Anthropic** | $$$$$ | Excellent | Fast | Cloud | Analysis-heavy tasks |
| **Google Gemini** | $$$ | Very Good | Fast | Cloud | Cost-conscious production |
| **Ollama** | Free | Good | Medium | Local | Development, privacy |

**üí° Cost Management Strategy:**
- Start with **Ollama** for development and testing (free)
- Use **Gemini** for cost-effective production deployment
- Switch to **OpenAI/Anthropic** for highest quality when needed
- Smart prompt engineering to minimize tokens across all providers
- Content filtering before AI processing

## üîç The Two-Stage Content Filtering System

Here's something crucial that most AI tutorials miss: **your AI model isn't just generating content - it's also acting as an intelligent filter and curator**. Understanding this is key to building a system that scales with content volume.

### Stage 1: Rule-Based Pre-Filtering (The Bouncer üëä)

This is what we built in previous chapters:
- **Engagement thresholds**: Remove low-engagement tweets
- **Spam detection**: Filter out noise patterns (RT spam, link-only posts)
- **Length requirements**: Ensure minimum content quality
- **Rate limiting**: Handle API constraints

```typescript
// Example from our TweetProcessor
const isQualityTweet = (tweet) => {
  // Rule-based filtering
  if (tweet.text.length < 20) return false;  // Too short
  if (tweet.engagement_score < 20) return false;  // Low engagement
  if (NOISE_PATTERNS.some(pattern => pattern.test(tweet.text))) return false;  // Spam
  return true;
};
```

**This stage removes obvious junk but passes through everything else.**

### Stage 2: AI-Powered Intelligent Filtering (The Curator)

This is where the magic happens. The AI model doesn't just summarize everything you feed it - **it intelligently selects, prioritizes, and curates the most relevant content**.

Here's what the AI is actually doing during content analysis:

```typescript
// What happens inside generateDigestContent()
const analysisPrompt = `
You are analyzing ${totalItems} pieces of content. Your job is to:

1. INTELLIGENTLY SELECT the most newsworthy and relevant items
2. IGNORE content that is repetitive, off-topic, or low-value
3. PRIORITIZE content that shows emerging trends or important developments
4. SYNTHESIZE insights from multiple sources when they discuss the same topic

Focus on HIGH-QUALITY content that provides genuine value to readers.
Do not include every item - be selective and focus on what truly matters.
`;
```

**The AI is making thousands of micro-decisions:**
- "This tweet about lunch is irrelevant - ignore it"
- "These 5 tweets are all about the same AI release - combine them into one insight"
- "This RSS article contradicts what Twitter users are saying - worth investigating"
- "This Telegram message has insider information - prioritize it"

### Why This Two-Stage System Works

**Token Economics Drive Intelligence:**
- With 100,000+ words of input but only 4,000 tokens for output, the AI *must* be selective
- This constraint forces the AI to act as an intelligent filter, not just a summarizer
- The AI naturally prioritizes higher-quality, more relevant content

**Example of AI Filtering in Action:**

```typescript
// Input: 200 tweets, 50 RSS articles, 100 Telegram messages
// Rule-based filter: Removes 150 low-quality items ‚Üí 200 items remain
// AI intelligent filter: Selects 30 most relevant items for final digest

// The AI might decide:
// ‚úÖ Include: Breaking AI research with high engagement
// ‚ùå Skip: Random crypto speculation with low engagement  
// ‚úÖ Include: Insider info from Telegram that aligns with Twitter trends
// ‚ùå Skip: Repetitive content already covered in better sources
// ‚úÖ Combine: Multiple tweets about same topic into single insight
```

**Quality Scoring Integration:**

The AI uses quality scores from our pre-filtering to make better decisions:

```typescript
// In prepareContentForAnalysis()
sections.push(`**Engagement:** ${tweet.engagement_score} (Quality: ${tweet.quality_score.toFixed(2)})`);
```

The AI sees these scores and weights content accordingly:
- High quality score = more likely to be included in final digest
- Multiple high-quality sources on same topic = combined into trend analysis
- Low quality score = might be mentioned briefly or ignored entirely

### The Real Power: Context-Aware Filtering

Unlike rule-based filters, the AI understands **context and relevance**:

```typescript
// Rule-based filter sees:
// Tweet A: "Just had coffee" (engagement: 50) ‚Üí PASS
// Tweet B: "OpenAI just released GPT-5" (engagement: 30) ‚Üí FAIL

// AI filter sees:
// Tweet A: Low relevance for tech digest ‚Üí IGNORE
// Tweet B: Highly relevant despite lower engagement ‚Üí PRIORITIZE
```

This is why AI costs are worth it - you're not just getting summarization, you're getting **intelligent content curation**.

## üéØ AI Data Types and Interfaces

Let's define our AI integration types:

```typescript
// types/ai.ts

export interface AIModelConfig {
  provider: 'openai' | 'anthropic' | 'google' | 'ollama';
  modelName: string;
  options: {
    temperature?: number;
    max_tokens?: number;
    top_p?: number;
    // OpenAI-specific options
    reasoning_effort?: 'low' | 'medium' | 'high'; // OpenAI o1 models
    // Anthropic-specific options
    thinking?: {  // Anthropic Claude thinking
      type: 'enabled' | 'disabled';
      budgetTokens?: number;
    };
    // Google Gemini-specific options
    safetySettings?: Array<{
      category: string;
      threshold: string;
    }>;
    // Ollama-specific options
    baseURL?: string; // Custom Ollama server URL
    keepAlive?: string; // Keep model loaded in memory
  };
}

export interface TokenUsage {
  prompt_tokens: number;
  completion_tokens: number;
  total_tokens: number;
  reasoning_tokens?: number; // For OpenAI o1 models
  cache_read_tokens?: number; // For Anthropic caching
}

export interface AIAnalysisRequest {
  content: ContentForAnalysis;
  analysisType: 'digest' | 'summary' | 'categorization' | 'sentiment';
  instructions?: string;
  outputFormat?: 'json' | 'markdown' | 'text';
}

export interface ContentForAnalysis {
  tweets?: AnalysisTweet[];
  telegram_messages?: AnalysisTelegramMessage[];
  rss_articles?: AnalysisRSSArticle[];
  timeframe: {
    from: string;
    to: string;
  };
  metadata: {
    total_sources: number;
    source_breakdown: {
      twitter: number;
      telegram: number;
      rss: number;
    };
  };
}

export interface AnalysisTweet {
  id: string;
  text: string;
  author: string;
  created_at: string;
  engagement_score: number;
  quality_score: number;
  url: string;
}

export interface AnalysisTelegramMessage {
  id: string;
  text: string;
  channel: string;
  author?: string;
  message_date: string;
  views: number;
  quality_score: number;
  url: string;
}

export interface AnalysisRSSArticle {
  id: string;
  title: string;
  content?: string;
  description: string;
  author?: string;
  published_at: string;
  source: string;
  quality_score: number;
  url: string;
}

export interface AIAnalysisResponse {
  analysis: DigestAnalysis;
  token_usage: TokenUsage;
  model_info: {
    provider: string;
    model: string;
    reasoning_time_ms?: number;
  };
  processing_time_ms: number;
}

export interface DigestAnalysis {
  title: string;
  executive_summary: string;
  key_insights: string[];
  trending_topics: TrendingTopic[];
  content_analysis: ContentAnalysis;
  recommendations: string[];
  confidence_score: number;
}

export interface TrendingTopic {
  topic: string;
  relevance_score: number;
  supporting_content: string[];
  trend_direction: 'rising' | 'stable' | 'declining';
}

export interface ContentAnalysis {
  sentiment: {
    overall: 'positive' | 'negative' | 'neutral';
    confidence: number;
    breakdown: {
      positive: number;
      neutral: number;
      negative: number;
    };
  };
  themes: {
    name: string;
    frequency: number;
    significance: number;
  }[];
  quality_distribution: {
    high_quality_percentage: number;
    average_engagement: number;
    content_diversity: number;
  };
}
```

## ü§ñ Building the AI Service

Let's first make sure we have all of the packages we need and update our scripts to test when complete:

**Package dependencies needed:**
```bash
# Core AI SDK
npm install ai

# Provider-specific packages
npm install @ai-sdk/openai @ai-sdk/anthropic @ai-sdk/google
npm install ollama-ai-provider

# TypeScript types
npm install --save-dev @types/node
```

**Package.json scripts to add:**
```json
{
  "scripts": {
    "test:ai": "npm run script scripts/test/test-ai.ts",
    "test:ai:openai": "npm run script scripts/test/test-ai.ts -- --provider=openai",
    "test:ai:claude": "npm run script scripts/test/test-ai.ts -- --provider=anthropic",
    "test:ai:gemini": "npm run script scripts/test/test-ai.ts -- --provider=google",
    "test:ai:ollama": "npm run script scripts/test/test-ai.ts -- --provider=ollama"
  }
}
```

**Environment variables needed:**

### For Cloud Providers:
```env
# OpenAI (Required for OpenAI models)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (Required for Claude models)  
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google (Required for Gemini models)
GOOGLE_GENERATIVE_AI_API_KEY=your_google_api_key_here
```

### For Local Provider (Ollama):
```env
# Optional: Custom Ollama server URL (defaults to http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# No API key needed for Ollama - it runs locally!
```

### Getting API Keys:

**Google Gemini API Key:**
1. Go to [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Click "Create API Key"
3. Copy the generated key to your `.env.local` file

**Ollama Setup:**
```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Or on macOS with Homebrew
brew install ollama

# Start Ollama server
ollama serve

# Pull a model (in another terminal)
ollama pull llama3.1:8b  # Good balance of performance/speed
ollama pull qwen2.5:7b   # Alternative option

# Verify it's working
ollama list
```

Now let's create our unified AI service that works with our A.I. providers:

```typescript
// lib/ai/ai-service.ts

import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google } from '@ai-sdk/google';
import { ollama } from 'ollama-ai-provider';
import { generateText, generateObject } from 'ai';
import { 
  AIModelConfig, 
  AIAnalysisRequest, 
  AIAnalysisResponse, 
  TokenUsage,
  DigestAnalysis 
} from '../../types/ai';
import { envConfig } from '../../config/environment';
import logger from '../logger';
import { ProgressTracker } from '../../utils/progress';

export class AIService {
  private static instance: AIService;
  private currentConfig: AIModelConfig;

  // Default configurations for all 4 providers
  private static readonly DEFAULT_OPENAI_CONFIG: AIModelConfig = {
    provider: 'openai',
    modelName: 'gpt-4o',
    options: {
      temperature: 0.7,
      max_tokens: 2000,
    }
  };

  private static readonly DEFAULT_ANTHROPIC_CONFIG: AIModelConfig = {
    provider: 'anthropic',
    modelName: 'claude-3-5-sonnet-20241022',
    options: {
      temperature: 0.7,
      max_tokens: 2000,
      thinking: {
        type: 'enabled',
        budgetTokens: 20000,
      }
    }
  };

  private static readonly DEFAULT_GOOGLE_CONFIG: AIModelConfig = {
    provider: 'google',
    modelName: 'gemini-1.5-pro',
    options: {
      temperature: 0.7,
      max_tokens: 2000,
      safetySettings: [
        {
          category: 'HARM_CATEGORY_HATE_SPEECH',
          threshold: 'BLOCK_MEDIUM_AND_ABOVE'
        },
        {
          category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
          threshold: 'BLOCK_MEDIUM_AND_ABOVE'
        }
      ]
    }
  };

  private static readonly DEFAULT_OLLAMA_CONFIG: AIModelConfig = {
    provider: 'ollama',
    modelName: 'llama3.1:8b', // 8B model for good balance of speed/quality
    options: {
      temperature: 0.7,
      max_tokens: 2000,
      baseURL: 'http://localhost:11434', // Default Ollama server
      keepAlive: '5m' // Keep model loaded for 5 minutes
    }
  };

  private constructor(config?: AIModelConfig) {
    this.currentConfig = config || AIService.DEFAULT_ANTHROPIC_CONFIG;
    this.validateConfiguration();
  }

  /**
   * Get singleton instance
   */
  public static getInstance(config?: AIModelConfig): AIService {
    if (!AIService.instance) {
      AIService.instance = new AIService(config);
    } else if (config) {
      AIService.instance.setConfig(config);
    }
    return AIService.instance;
  }

  /**
   * Set AI model configuration
   */
  public setConfig(config: AIModelConfig): void {
    this.currentConfig = config;
    this.validateConfiguration();
    logger.info(`AI model configured: ${config.provider}:${config.modelName}`);
  }

  /**
   * Switch to OpenAI
   */
  public useOpenAI(modelName?: string): void {
    this.setConfig({
      ...AIService.DEFAULT_OPENAI_CONFIG,
      modelName: modelName || AIService.DEFAULT_OPENAI_CONFIG.modelName
    });
  }

  /**
   * Switch to Anthropic Claude
   */
  public useClaude(modelName?: string): void {
    this.setConfig({
      ...AIService.DEFAULT_ANTHROPIC_CONFIG,
      modelName: modelName || AIService.DEFAULT_ANTHROPIC_CONFIG.modelName
    });
  }

  /**
   * Switch to Google Gemini
   */
  public useGemini(modelName?: string): void {
    this.setConfig({
      ...AIService.DEFAULT_GOOGLE_CONFIG,
      modelName: modelName || AIService.DEFAULT_GOOGLE_CONFIG.modelName
    });
  }

  /**
   * Switch to Ollama (local)
   */
  public useOllama(modelName?: string, baseURL?: string): void {
    this.setConfig({
      ...AIService.DEFAULT_OLLAMA_CONFIG,
      modelName: modelName || AIService.DEFAULT_OLLAMA_CONFIG.modelName,
      options: {
        ...AIService.DEFAULT_OLLAMA_CONFIG.options,
        ...(baseURL && { baseURL })
      }
    });
  }

  /**
   * Main analysis method - analyzes content and generates insights
   */
  async analyzeContent(request: AIAnalysisRequest): Promise<AIAnalysisResponse> {
    const startTime = Date.now();
    const progress = new ProgressTracker({
      total: 4,
      label: `AI Analysis (${this.currentConfig.provider}:${this.currentConfig.modelName})`
    });

    try {
      // Step 1: Prepare content for analysis
      progress.update(1, { step: 'Preparing content' });
      const preparedContent = this.prepareContentForAnalysis(request.content);
      
      // Step 2: Generate analysis prompt
      progress.update(2, { step: 'Generating prompt' });
      const analysisPrompt = this.buildAnalysisPrompt(request, preparedContent);
      
      // Step 3: Call AI model
      progress.update(3, { step: 'AI processing' });
      const aiResponse = await this.callAIModel(analysisPrompt, request.outputFormat);
      
      // Step 4: Process and validate response
      progress.update(4, { step: 'Processing response' });
      const analysis = this.parseAndValidateResponse(aiResponse.text);
      
      const processingTime = Date.now() - startTime;
      progress.complete(`Analysis completed in ${(processingTime / 1000).toFixed(2)}s`);

      return {
        analysis,
        token_usage: this.extractTokenUsage(aiResponse),
        model_info: {
          provider: this.currentConfig.provider,
          model: this.currentConfig.modelName,
          reasoning_time_ms: aiResponse.reasoning_time_ms
        },
        processing_time_ms: processingTime
      };

    } catch (error: any) {
      progress.fail(`AI analysis failed: ${error.message}`);
      logger.error('AI analysis failed', { 
        error: error.message,
        provider: this.currentConfig.provider,
        model: this.currentConfig.modelName 
      });
      throw error;
    }
  }

  /**
   * Prepare content for AI analysis with quality signals for intelligent filtering
   * 
   * This method doesn't just format content - it provides the AI with key signals
   * to make intelligent filtering decisions during content analysis.
   */
  private prepareContentForAnalysis(content: any): string {
    const sections: string[] = [];

    // Add tweets with quality signals for AI filtering
    if (content.tweets?.length > 0) {
      sections.push('## TWITTER/X CONTENT');
      sections.push(`*Note: ${content.tweets.length} tweets passed rule-based filtering. Focus on highest quality and most relevant items.*`);
      sections.push('');
      
      content.tweets.forEach((tweet: any, index: number) => {
        sections.push(`### Tweet ${index + 1}`);
        sections.push(`**Author:** @${tweet.author}`);
        sections.push(`**Date:** ${tweet.created_at}`);
        
        // Quality signals that guide AI filtering decisions
        sections.push(`**Engagement:** ${tweet.engagement_score} (Quality: ${tweet.quality_score.toFixed(2)})`);
        sections.push(`**Priority:** ${tweet.engagement_score > 100 ? 'HIGH' : tweet.engagement_score > 50 ? 'MEDIUM' : 'LOW'}`);
        
        sections.push(`**Content:** ${tweet.text}`);
        sections.push(`**URL:** ${tweet.url}`);
        sections.push('');
      });
    }

    // Add Telegram messages with filtering guidance
    if (content.telegram_messages?.length > 0) {
      sections.push('## TELEGRAM CONTENT');
      sections.push(`*Note: ${content.telegram_messages.length} messages from insider channels. Prioritize unique insights and breaking news.*`);
      sections.push('');
      
      content.telegram_messages.forEach((msg: any, index: number) => {
        sections.push(`### Message ${index + 1}`);
        sections.push(`**Channel:** ${msg.channel}`);
        sections.push(`**Author:** ${msg.author || 'Unknown'}`);
        sections.push(`**Date:** ${msg.message_date}`);
        
        // Quality signals for AI filtering
        sections.push(`**Views:** ${msg.views} (Quality: ${msg.quality_score.toFixed(2)})`);
        sections.push(`**Signal Strength:** ${msg.views > 1000 ? 'STRONG' : msg.views > 500 ? 'MEDIUM' : 'WEAK'}`);
        
        sections.push(`**Content:** ${msg.text}`);
        sections.push(`**URL:** ${msg.url}`);
        sections.push('');
      });
    }

    // Add RSS articles with relevance scoring
    if (content.rss_articles?.length > 0) {
      sections.push('## RSS ARTICLES');
      sections.push(`*Note: ${content.rss_articles.length} articles from news sources. Focus on breaking news and unique analysis.*`);
      sections.push('');
      
      content.rss_articles.forEach((article: any, index: number) => {
        sections.push(`### Article ${index + 1}`);
        sections.push(`**Title:** ${article.title}`);
        sections.push(`**Source:** ${article.source}`);
        sections.push(`**Author:** ${article.author || 'Unknown'}`);
        sections.push(`**Date:** ${article.published_at}`);
        
        // Quality signals for AI filtering decisions
        sections.push(`**Quality Score:** ${article.quality_score.toFixed(2)}`);
        sections.push(`**Content Type:** ${article.quality_score > 0.8 ? 'PREMIUM ANALYSIS' : article.quality_score > 0.6 ? 'STANDARD NEWS' : 'BRIEF UPDATE'}`);
        
        sections.push(`**Summary:** ${article.description}`);
        if (article.content) {
          sections.push(`**Content:** ${article.content.substring(0, 1000)}${article.content.length > 1000 ? '...' : ''}`);
        }
        sections.push(`**URL:** ${article.url}`);
        sections.push('');
      });
    }

    // Add metadata
    sections.push('## METADATA');
    sections.push(`**Timeframe:** ${content.timeframe.from} to ${content.timeframe.to}`);
    sections.push(`**Total Sources:** ${content.metadata.total_sources}`);
    sections.push(`**Source Breakdown:**`);
    sections.push(`- Twitter: ${content.metadata.source_breakdown.twitter} items`);
    sections.push(`- Telegram: ${content.metadata.source_breakdown.telegram} items`);
    sections.push(`- RSS: ${content.metadata.source_breakdown.rss} items`);

    return sections.join('\n');
  }

  /**
   * Call the appropriate AI model based on current configuration
   */
  private async callAIModel(prompt: string, outputFormat: string = 'json'): Promise<any> {
    const { provider, modelName, options } = this.currentConfig;

    const baseOptions = {
      temperature: options.temperature || 0.7,
      maxTokens: options.max_tokens || 2000,
    };

    try {
      switch (provider) {
        case 'openai':
          return await generateText({
            model: openai(modelName),
            prompt,
            ...baseOptions,
            ...(options.reasoning_effort && { reasoningEffort: options.reasoning_effort })
          });

        case 'anthropic':
          return await generateText({
            model: anthropic(modelName),
            prompt,
            ...baseOptions,
            ...(options.thinking && {
              experimental_toolCallMode: 'json',
              experimental_thinking: options.thinking.type === 'enabled',
              ...(options.thinking.budgetTokens && {
                experimental_thinkingBudgetTokens: options.thinking.budgetTokens
              })
            })
          });

        case 'google':
          return await generateText({
            model: google(modelName),
            prompt,
            ...baseOptions,
            ...(options.safetySettings && { safetySettings: options.safetySettings })
          });

        case 'ollama':
          return await generateText({
            model: ollama(modelName),
            prompt,
            ...baseOptions
          });

        default:
          throw new Error(`Unsupported AI provider: ${provider}`);
      }
    } catch (error: any) {
      // Add provider-specific error handling
      if (provider === 'ollama' && error?.message?.includes('ECONNREFUSED')) {
        throw new Error('Ollama server not running. Start it with: ollama serve');
      }
      if (provider === 'google' && error?.message?.includes('API_KEY_INVALID')) {
        throw new Error('Invalid Google API key. Check GOOGLE_GENERATIVE_AI_API_KEY environment variable');
      }
      throw error;
    }
  }

  /**
   * Build analysis prompt that guides AI filtering and content selection
   * 
   * This prompt is crucial - it instructs the AI to act as an intelligent filter,
   * not just a summarizer. The AI will select, prioritize, and curate content.
   */
  private buildAnalysisPrompt(request: AIAnalysisRequest, preparedContent: string): string {
    const baseInstructions = `You are an expert content analyst and curator specializing in technology, finance, and current events. 

Your job has TWO phases:
1. INTELLIGENT FILTERING: Select only the most valuable, relevant, and newsworthy content
2. ANALYSIS: Generate actionable insights from your curated selection

CONTENT CURATION GUIDELINES:
- IGNORE repetitive, off-topic, or low-value content
- PRIORITIZE breaking news, unique insights, and emerging trends
- COMBINE multiple sources discussing the same topic into single insights
- FOCUS on content with high engagement scores and quality ratings
- SELECT content that provides genuine value to readers

ANALYSIS REQUIREMENTS:
1. Focus on the most significant trends and patterns from your curated selection
2. Prioritize high-quality, high-engagement content you've selected
3. Identify emerging themes from your filtered content
4. Provide balanced, objective analysis based on your curation
5. Include confidence levels for your assessments
6. Cite specific examples from the content you chose to include

OUTPUT FORMAT: Return a valid JSON object with the following structure:
{
  "title": "Concise title summarizing the key theme (max 100 chars)",
  "executive_summary": "3-4 sentence overview of the most important findings",
  "key_insights": ["Array of 3-5 key insights, each 1-2 sentences"],
  "trending_topics": [
    {
      "topic": "Topic name",
      "relevance_score": 0.8,
      "supporting_content": ["Brief quotes or references"],
      "trend_direction": "rising|stable|declining"
    }
  ],
  "content_analysis": {
    "sentiment": {
      "overall": "positive|negative|neutral",
      "confidence": 0.85,
      "breakdown": {"positive": 60, "neutral": 30, "negative": 10}
    },
    "themes": [
      {"name": "Theme name", "frequency": 5, "significance": 0.9}
    ],
    "quality_distribution": {
      "high_quality_percentage": 75,
      "average_engagement": 150,
      "content_diversity": 0.8
    }
  },
  "recommendations": ["Array of 2-4 actionable recommendations"],
  "confidence_score": 0.85
}`;

    // Add specific instructions based on analysis type
    let specificInstructions = '';
    switch (request.analysisType) {
      case 'digest':
        specificInstructions = `
DIGEST-SPECIFIC INSTRUCTIONS:
- Create a comprehensive daily digest format
- Highlight breaking news and significant developments
- Connect related stories across different sources
- Identify market implications and business opportunities
- Focus on actionable intelligence for decision-makers`;
        break;
      
      case 'summary':
        specificInstructions = `
SUMMARY-SPECIFIC INSTRUCTIONS:
- Provide concise, factual summaries
- Maintain key details and context
- Avoid speculation or analysis beyond the source material
- Focus on information density and clarity`;
        break;
      
      case 'sentiment':
        specificInstructions = `
SENTIMENT-SPECIFIC INSTRUCTIONS:
- Perform detailed sentiment analysis
- Identify emotional tone and market sentiment
- Analyze sentiment trends over time
- Provide confidence levels for sentiment assessments`;
        break;
    }

    if (request.instructions) {
      specificInstructions += `\n\nADDITIONAL INSTRUCTIONS:\n${request.instructions}`;
    }

    return `${baseInstructions}\n${specificInstructions}\n\nCONTENT TO ANALYZE:\n\n${preparedContent}`;
  }



  /**
   * Parse and validate AI response
   */
  private parseAndValidateResponse(responseText: string): DigestAnalysis {
    try {
      // Try to extract JSON from the response
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        throw new Error('No JSON found in response');
      }

      const parsed = JSON.parse(jsonMatch[0]);
      
      // Validate required fields
      const required = ['title', 'executive_summary', 'key_insights', 'trending_topics', 'content_analysis', 'recommendations', 'confidence_score'];
      for (const field of required) {
        if (!(field in parsed)) {
          throw new Error(`Missing required field: ${field}`);
        }
      }

      // Validate structure
      if (!Array.isArray(parsed.key_insights)) {
        throw new Error('key_insights must be an array');
      }
      
      if (!Array.isArray(parsed.trending_topics)) {
        throw new Error('trending_topics must be an array');
      }

      if (!parsed.content_analysis.sentiment) {
        throw new Error('content_analysis.sentiment is required');
      }

      return parsed as DigestAnalysis;

    } catch (error: any) {
      logger.error('Failed to parse AI response', { error: error.message, response: responseText.substring(0, 500) });
      
      // Fallback response
      return {
        title: 'Analysis Failed',
        executive_summary: 'Unable to process the content due to parsing errors.',
        key_insights: ['Content analysis could not be completed'],
        trending_topics: [],
        content_analysis: {
          sentiment: {
            overall: 'neutral',
            confidence: 0.0,
            breakdown: { positive: 33, neutral: 33, negative: 33 }
          },
          themes: [],
          quality_distribution: {
            high_quality_percentage: 0,
            average_engagement: 0,
            content_diversity: 0
          }
        },
        recommendations: ['Please review the content and try again'],
        confidence_score: 0.0
      };
    }
  }

  /**
   * Extract token usage from AI response
   */
  private extractTokenUsage(response: any): TokenUsage {
    const usage = response.usage;
    if (!usage) {
      return { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 };
    }

    return {
      prompt_tokens: usage.promptTokens || 0,
      completion_tokens: usage.completionTokens || 0,
      total_tokens: usage.totalTokens || 0,
      reasoning_tokens: usage.reasoningTokens,
      cache_read_tokens: usage.cacheReadTokens
    };
  }

  /**
   * Validate configuration
   */
  private validateConfiguration(): void {
    const { provider, modelName } = this.currentConfig;
    
    if (provider === 'openai' && !process.env.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY environment variable is required for OpenAI');
    }
    
    if (provider === 'anthropic' && !process.env.ANTHROPIC_API_KEY) {
      throw new Error('ANTHROPIC_API_KEY environment variable is required for Anthropic');
    }

    if (provider === 'google' && !process.env.GOOGLE_GENERATIVE_AI_API_KEY) {
      throw new Error('GOOGLE_GENERATIVE_AI_API_KEY environment variable is required for Google Gemini');
    }

    // Note: Ollama doesn't require API key validation as it's a local service

    logger.debug('AI configuration validated', { provider, modelName });
  }

  /**
   * Get current configuration
   */
  public getConfig(): AIModelConfig {
    return { ...this.currentConfig };
  }

  /**
   * Test AI connection
   */
  async testConnection(): Promise<boolean> {
    try {
      const testRequest: AIAnalysisRequest = {
        content: {
          tweets: [{
            id: 'test',
            text: 'This is a test tweet about AI technology.',
            author: 'test_user',
            created_at: new Date().toISOString(),
            engagement_score: 10,
            quality_score: 0.8,
            url: 'https://twitter.com/test'
          }],
          timeframe: {
            from: new Date().toISOString(),
            to: new Date().toISOString()
          },
          metadata: {
            total_sources: 1,
            source_breakdown: { twitter: 1, telegram: 0, rss: 0 }
          }
        },
        analysisType: 'summary'
      };

      const response = await this.analyzeContent(testRequest);
      logger.info(`AI connection test successful: ${this.currentConfig.provider}:${this.currentConfig.modelName}`);
      logger.info(response);
      return true;

    } catch (error) {
      logger.error(`AI connection test failed: ${this.currentConfig.provider}:${this.currentConfig.modelName}`, error);
      return false;
    }
  }
}

```

## üß™ Testing Your AI Integration

Let's create a comprehensive test for our AI service:

```typescript
// scripts/test/test-ai.ts

import { config } from 'dotenv';
config({ path: '.env.local' });

import { AIService } from '../../lib/ai/ai-service';
import { AIAnalysisRequest } from '../../types/ai';
import logger from '../../lib/logger';

// Command line argument parsing
const args = process.argv.slice(2);
const providerArg = args.find(arg => arg.startsWith('--provider='));
const selectedProvider = providerArg ? providerArg.split('=')[1] : 'all';

// Provider configurations for testing
const PROVIDER_CONFIGS = {
  openai: {
    name: 'OpenAI',
    method: 'useOpenAI',
    model: 'gpt-4o',
    envVar: 'OPENAI_API_KEY',
    costRates: { prompt: 0.0000025, completion: 0.00001 }
  },
  anthropic: {
    name: 'Anthropic Claude',
    method: 'useClaude', 
    model: 'claude-3-5-sonnet-20241022',
    envVar: 'ANTHROPIC_API_KEY',
    costRates: { prompt: 0.000003, completion: 0.000015 }
  },
  google: {
    name: 'Google Gemini',
    method: 'useGemini',
    model: 'gemini-1.5-pro',
    envVar: 'GOOGLE_GENERATIVE_AI_API_KEY',
    costRates: { prompt: 0.00000125, completion: 0.000005 }
  },
  ollama: {
    name: 'Ollama (Local)',
    method: 'useOllama', 
    model: 'llama3.1:8b',
    envVar: null, // No API key required
    costRates: { prompt: 0, completion: 0 } // Local model, no cost
  }
} as const;

async function testAIIntegration() {
  console.log('ü§ñ Testing AI Integration...\n');

  if (selectedProvider !== 'all') {
    console.log(`üéØ Testing specific provider: ${selectedProvider.toUpperCase()}\n`);
  }

  try {
    const aiService = AIService.getInstance();
    const testResults: Array<{
      provider: string;
      success: boolean;
      response?: any;
      error?: string;
      cost?: number;
    }> = [];

    // Determine which providers to test
    const providersToTest = selectedProvider === 'all' 
      ? Object.keys(PROVIDER_CONFIGS)
      : [selectedProvider];

    // Validate provider selection
    for (const provider of providersToTest) {
      if (!(provider in PROVIDER_CONFIGS)) {
        console.error(`‚ùå Unknown provider: ${provider}`);
        console.log('Available providers: openai, anthropic, google, ollama');
        process.exit(1);
      }
    }

    console.log('üìã Environment Check:');
    let hasAllRequiredKeys = true;
    
    for (const provider of providersToTest) {
      const config = PROVIDER_CONFIGS[provider as keyof typeof PROVIDER_CONFIGS];
      if (config.envVar) {
        const hasKey = !!process.env[config.envVar];
        console.log(`   ${config.name}: ${hasKey ? '‚úÖ' : '‚ùå'} (${config.envVar})`);
        if (!hasKey) hasAllRequiredKeys = false;
      } else {
        console.log(`   ${config.name}: ‚úÖ (No API key required)`);
      }
    }

    if (!hasAllRequiredKeys) {
      console.log('\nüí° Missing API keys. Add them to .env.local:');
      console.log('   OPENAI_API_KEY=your_openai_key');
      console.log('   ANTHROPIC_API_KEY=your_anthropic_key');  
      console.log('   GOOGLE_GENERATIVE_AI_API_KEY=your_google_key');
      console.log('   (Ollama requires no API key, just local server)');
    }

    // Test each provider
    for (const [index, provider] of providersToTest.entries()) {
      const config = PROVIDER_CONFIGS[provider as keyof typeof PROVIDER_CONFIGS];
      
      console.log(`\n${index + 1}. Testing ${config.name} Connection:`);
      
      try {
        // Configure the service for this provider
        (aiService as any)[config.method](config.model);
        
        // Test connection
        const connected = await aiService.testConnection();
        if (connected) {
          console.log(`‚úÖ ${config.name} connection successful`);
        } else {
          console.log(`‚ùå ${config.name} connection failed`);
          testResults.push({ provider, success: false, error: 'Connection test failed' });
          continue;
        }

        // Test content analysis if connection successful
        console.log(`   Testing content analysis...`);
        const analysisResponse = await aiService.analyzeContent(getTestContent());
        
        console.log(`‚úÖ ${config.name} Analysis Complete:`);
        console.log(`   Title: "${analysisResponse.analysis.title}"`);
        console.log(`   Key Insights: ${analysisResponse.analysis.key_insights.length} insights`);
        console.log(`   Trending Topics: ${analysisResponse.analysis.trending_topics.length} topics`);
        console.log(`   Confidence: ${analysisResponse.analysis.confidence_score.toFixed(2)}`);
        console.log(`   Tokens: ${analysisResponse.token_usage.total_tokens} (Prompt: ${analysisResponse.token_usage.prompt_tokens}, Completion: ${analysisResponse.token_usage.completion_tokens})`);
        console.log(`   Processing Time: ${(analysisResponse.processing_time_ms / 1000).toFixed(2)}s`);
        
        // Calculate cost
        const cost = calculateCost(analysisResponse.token_usage, config.costRates);
        if (cost > 0) {
          console.log(`   Estimated Cost: $${cost.toFixed(6)}`);
        } else {
          console.log(`   Cost: Free (local model)`);
        }

        testResults.push({ 
          provider, 
          success: true, 
          response: analysisResponse,
          cost 
        });

      } catch (error: any) {
        console.log(`‚ùå ${config.name} test failed: ${error.message}`);
        
        // Provider-specific error guidance
        if (provider === 'ollama' && error.message.includes('Ollama server not running')) {
          console.log('   üí° Start Ollama server with: ollama serve');
          console.log('   üí° Then pull the model with: ollama pull llama3.1:8b');
        } else if (error.message.includes('API_KEY') || error.message.includes('API key')) {
          console.log(`   üí° Check your ${config.envVar} environment variable`);
        }
        
        testResults.push({ 
          provider, 
          success: false, 
          error: error.message 
        });
      }
    }

    // Summary
    console.log('\nüìä Test Summary:');
    const successful = testResults.filter(r => r.success);
    const failed = testResults.filter(r => !r.success);
    
    console.log(`   ‚úÖ Successful: ${successful.length}/${testResults.length}`);
    console.log(`   ‚ùå Failed: ${failed.length}/${testResults.length}`);

    if (successful.length > 0) {
      console.log('\nüí∞ Cost Comparison (for this test):');
      successful.forEach(result => {
        const config = PROVIDER_CONFIGS[result.provider as keyof typeof PROVIDER_CONFIGS];
        if (result.cost! > 0) {
          console.log(`   ${config.name}: $${result.cost!.toFixed(6)}`);
        } else {
          console.log(`   ${config.name}: Free (local)`);
        }
      });
    }

    if (successful.length >= 2) {
      console.log('\nüîç Response Quality Comparison:');
      successful.slice(0, 2).forEach(result => {
        const config = PROVIDER_CONFIGS[result.provider as keyof typeof PROVIDER_CONFIGS];
        console.log(`   ${config.name}: "${result.response!.analysis.executive_summary.substring(0, 100)}..."`);
      });
    }

    if (failed.length > 0) {
      console.log('\n‚ùå Failed Providers:');
      failed.forEach(result => {
        const config = PROVIDER_CONFIGS[result.provider as keyof typeof PROVIDER_CONFIGS];
        console.log(`   ${config.name}: ${result.error}`);
      });
    }

    console.log('\nüéâ AI integration test completed!');
    
    if (successful.length > 0) {
      console.log('\nüí° Provider Recommendations:');
      console.log('   - OpenAI: Fast, cost-effective, good general performance');
      console.log('   - Anthropic: Best for complex analysis and reasoning');
      console.log('   - Google Gemini: Good balance of speed and quality');
      console.log('   - Ollama: Free local inference, privacy-focused');
    }

    // Exit with error code if all tests failed
    if (successful.length === 0) {
      process.exit(1);
    }

  } catch (error: any) {
    logger.error('AI integration test failed', error);
    console.error('\n‚ùå Test failed:', error.message);
    process.exit(1);
  }
}

function getTestContent(): AIAnalysisRequest {
  return {
    content: {
      tweets: [
        {
          id: 'tweet1',
          text: 'Breaking: New AI model shows unprecedented capabilities in reasoning and mathematics',
          author: 'AI_News',
          created_at: '2024-01-15T10:00:00Z',
          engagement_score: 150,
          quality_score: 0.9,
          url: 'https://twitter.com/AI_News/status/1'
        },
        {
          id: 'tweet2', 
          text: 'The future of work is changing rapidly with AI automation. Companies need to adapt now.',
          author: 'TechExpert',
          created_at: '2024-01-15T11:00:00Z',
          engagement_score: 85,
          quality_score: 0.8,
          url: 'https://twitter.com/TechExpert/status/2'
        }
      ],
      rss_articles: [
        {
          id: 'article1',
          title: 'The Rise of Large Language Models in Enterprise',
          description: 'How companies are integrating AI into their workflows',
          content: 'Large language models are transforming how businesses operate...',
          author: 'Jane Smith',
          published_at: '2024-01-15T09:00:00Z',
          source: 'TechCrunch',
          quality_score: 0.95,
          url: 'https://techcrunch.com/article1'
        }
      ],
      timeframe: {
        from: '2024-01-15T00:00:00Z',
        to: '2024-01-15T23:59:59Z'
      },
      metadata: {
        total_sources: 3,
        source_breakdown: {
          twitter: 2,
          telegram: 0,
          rss: 1
        }
      }
    },
    analysisType: 'digest'
  };
}

function calculateCost(tokenUsage: any, rates: { prompt: number; completion: number }): number {
  return (tokenUsage.prompt_tokens * rates.prompt) + (tokenUsage.completion_tokens * rates.completion);
}

testAIIntegration();
```

## üß™ Testing All AI Providers

Test each provider to find what works best for your use case:

### Quick Provider Tests:
```bash
# Test OpenAI (requires API key)
npm run test:ai:openai

# Test Claude (requires API key)  
npm run test:ai:claude

# Test Gemini (requires API key, cheapest cloud option)
npm run test:ai:gemini

# Test Ollama (free, requires local setup)
npm run test:ai:ollama
```

### Usage Examples in Your Code:

```typescript
// Example: Using different providers for different tasks
import { AIService } from '../lib/ai/ai-service';

const aiService = AIService.getInstance();

// Use Ollama for development/testing (free)
if (process.env.NODE_ENV === 'development') {
  await aiService.useOllama('llama3.1:8b');
}

// Use Gemini for cost-effective production
else if (process.env.AI_BUDGET === 'low') {
  await aiService.useGemini('gemini-1.5-pro');
}

// Use Claude for highest quality analysis
else if (process.env.AI_QUALITY === 'premium') {
  await aiService.useClaude('claude-3-5-sonnet-20241022');
}

// Use OpenAI for balanced performance
else {
  await aiService.useOpenAI('gpt-4o');
}

// Now generate your digest
const result = await aiService.analyzeContent(contentData);
```

### Performance & Cost Comparison:

Based on our testing with typical digest content:

| Provider | Cost/Digest | Speed | Quality | Best For |
|----------|-------------|-------|---------|----------|
| **Ollama** | Free | Medium | Good | Development, privacy |
| **Gemini** | $0.05-0.25 | Fast | Very Good | Production, budget-conscious |
| **Claude** | $0.15-0.75 | Fast | Excellent | Analysis-heavy, premium |
| **OpenAI** | $0.10-0.50 | Fast | Excellent | Balanced, reliable |

### Troubleshooting:

**Ollama Issues:**
```bash
# If Ollama fails to connect
ollama serve  # Make sure server is running

# If model not found
ollama pull llama3.1:8b  # Download the model first
```

**API Key Issues:**
```bash
# Verify your environment variables
echo $OPENAI_API_KEY
echo $ANTHROPIC_API_KEY  
echo $GOOGLE_GENERATIVE_AI_API_KEY

# Test API connectivity
curl -H "Authorization: Bearer $OPENAI_API_KEY" https://api.openai.com/v1/models
```

## üéØ What We've Accomplished

You now have a sophisticated AI integration system that:

‚úÖ **Supports multiple AI providers** (OpenAI and Anthropic)  
‚úÖ **Handles structured content analysis** with comprehensive prompts  
‚úÖ **Provides intelligent cost management** with usage tracking  
‚úÖ **Includes robust error handling** and fallback responses  
‚úÖ **Offers configurable model selection** based on use case  
‚úÖ **Delivers structured, actionable insights** from raw content  

### üîç Pro Tips & Common Pitfalls

**üí° Pro Tip:** Start with smaller content batches to understand token usage patterns before scaling up.

**‚ö†Ô∏è Common Pitfall:** Don't send too much content at once. AI models have context limits, and costs scale with token usage.

**üîß Performance Tip:** Experiment with the models to fit your performance needs. For example, some argue Claude is better for complex analysis and reasoning tasks, where OpenAI is faster, more straightforward processing.

**üí∞ Cost Optimization:** Pre-filter low-quality content before sending to AI models to minimize token usage.

---

### üìã Complete Code Summary - Chapter 7

**Core AI Service:**
```typescript
// lib/ai/ai-service.ts - Unified AI service supporting OpenAI and Anthropic
// types/ai.ts - Comprehensive AI integration types
```

**Testing:**
```typescript
// scripts/test/test-ai.ts - Comprehensive AI integration test with cost analysis
```

## üîÑ Complete AI Filtering Workflow Summary

Now you understand the full content pipeline in your digest system:

### The Complete Journey: Raw Data ‚Üí Curated Insights

```typescript
// 1. Data Collection (Chapters 4-6)
const rawTweets = await fetchTweets(); // 500+ tweets
const rawTelegram = await fetchTelegram(); // 100+ messages  
const rawRSS = await fetchRSS(); // 50+ articles

// 2. Rule-Based Pre-Filtering (Chapter 4-6)
const qualityTweets = tweetProcessor.filterTweets(rawTweets); // ‚Üí 100 tweets
const qualityTelegram = filterTelegramMessages(rawTelegram); // ‚Üí 30 messages
const qualityRSS = filterRSSArticles(rawRSS); // ‚Üí 20 articles

// 3. AI Intelligent Filtering & Analysis (Chapter 7)
const digest = await aiService.generateDigestContent(
  qualityTweets,    // AI selects ~15 most relevant tweets
  qualityTelegram,  // AI selects ~8 most valuable messages  
  qualityRSS        // AI selects ~5 most important articles
);
// Result: 28 pieces of high-value content ‚Üí 1 actionable digest
```

### Why This Two-Stage System is Powerful

**Without AI Filtering:** You'd get overwhelming, repetitive summaries of everything
**With AI Filtering:** You get curated, synthesized insights from the most valuable content

**Cost-Benefit Analysis (with 4 provider options):**
- **Input:** 650+ raw items ‚Üí 150 quality items ‚Üí 28 curated insights
- **Cost Range:** 
  - Ollama: Free (after setup)
  - Gemini: ~$0.05-0.25 per digest  
  - OpenAI: ~$0.10-0.50 per digest
  - Claude: ~$0.15-0.75 per digest
- **Value:** Saves 2-3 hours of manual content review daily
- **ROI:** 500-1000x time savings (even with premium providers)

The AI doesn't just summarize - it **thinks, selects, combines, and prioritizes** like a human analyst would, regardless of which provider you choose.

**Next up:** Chapter 8 will show you advanced AI techniques - building sophisticated prompts, handling different content types, and creating specialized analysis workflows. We'll also explore cost optimization strategies and advanced model features!

---

*Ready to master advanced AI techniques? Chapter 8 will teach you prompt engineering secrets and advanced analysis patterns that separate amateur AI integrations from professional-grade systems! üß†*